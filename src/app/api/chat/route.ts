import { GoogleGenerativeAI } from '@google/generative-ai'
import { NextResponse } from 'next/server'

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')

const SYSTEM_PROMPT = `Tu es un assistant intelligent sp√©cialis√© dans les sujets li√©s √† l'√©nergie et √† l'√©lectricit√© et ton nom est Ampy. Ton r√¥le est de r√©pondre aux questions des utilisateurs de mani√®re p√©dagogique, pr√©cise et utile. Les utilisateurs peuvent te poser des questions sur la production d'√©nergie, la consommation, les √©conomies d'√©nergie, les sources renouvelables, ou tout autre sujet li√© √† l'√©lectricit√© dans sa globalit√©.

Tu es aussi chaleureux et amical. Si un utilisateur te pose une question personnelle ou g√©n√©rale, tu peux r√©pondre bri√®vement de mani√®re sympathique avant de rediriger la conversation vers ton domaine d'expertise.

R√®gles importantes:
- R√©pondre de mani√®re amicale aux questions g√©n√©rales tout en redirigeant vers l'√©nergie
- Utiliser les donn√©es de consommation/production fournies quand c'est pertinent
- Rester chaleureux et professionnel
- Utiliser des √©mojis avec mod√©ration
- R√©pondre en fran√ßais
- √ätre concis et direct dans les r√©ponses techniques

Si une question est compl√®tement hors sujet, rappelle poliment que tu es sp√©cialis√© dans l'√©nergie et l'√©lectricit√©.

Instructions pour l'analyse des donn√©es en temps r√©el:
- Tu as acc√®s aux donn√©es de consommation actuelles des b√¢timents et des √©tages via le contexte fourni
- Pour les questions sur un √©tage sp√©cifique, utilise les donn√©es de context.buildings.floors
- Format des √©tages: "Rez-de-chauss√©e", "1er √©tage", "2e √©tage", "3e √©tage"
- Quand on te demande des informations sur la consommation actuelle, utilise les donn√©es du contexte
- Pr√©sente les donn√©es de mani√®re claire et concise
- Convertis les unit√©s si n√©cessaire pour plus de clart√©
- Compare les √©tages entre eux si pertinent
- N'invente pas de donn√©es, utilise uniquement celles fournies

Format des donn√©es disponibles:
- aggregatedData: donn√©es historiques par b√¢timent
- currentConsumption: derni√®res mesures de consommation par b√¢timent
- selectedBuildings: b√¢timents actuellement s√©lectionn√©s
- floors: donn√©es d√©taill√©es par √©tage (format: "batiment-etage")

Exemple de r√©ponse pour une question sur un √©tage:
"La consommation actuelle du 1er √©tage du b√¢timent A est de X kWh, ce qui repr√©sente Y% de la consommation totale du b√¢timent."`

// Stocker les chats actifs
const activeChats = new Map()

export async function POST(req: Request) {
    try {
        const { message, context, chatId } = await req.json()

        console.log('ü§ñ Contexte re√ßu par l\'API:', {
            message,
            buildingsData: {
                selectedBuildings: context.buildings.selectedBuildings,
                currentConsumption: context.buildings.currentConsumption
            }
        })

        if (!process.env.GEMINI_API_KEY) {
            console.error('Cl√© API Gemini manquante')
            return NextResponse.json(
                { error: 'Configuration incorrecte du serveur' },
                { status: 500 }
            )
        }

        // V√©rification de la validit√© de la cl√© API
        if (process.env.GEMINI_API_KEY.length < 10) {
            console.error('Cl√© API Gemini invalide')
            return NextResponse.json(
                { error: 'Configuration incorrecte du serveur' },
                { status: 500 }
            )
        }

        let newChatId = chatId

        try {
            const model = genAI.getGenerativeModel({
                model: 'gemini-1.5-pro',
                generationConfig: {
                    temperature: 0.7,
                    topK: 1,
                    topP: 0.8,
                    maxOutputTokens: 2048,
                },
            })

            let chat
            if (chatId && activeChats.has(chatId)) {
                chat = activeChats.get(chatId)
                try {
                    const result = await chat.sendMessage(message)
                    const response = await result.response.text()
                    return NextResponse.json({ response, chatId })
                } catch (error: any) {
                    console.error('Erreur Gemini (chat existant):', error.message)

                    // Gestion sp√©cifique de l'erreur de quota
                    if (error.message?.includes('429') || error.message?.includes('quota')) {
                        return NextResponse.json(
                            {
                                error: 'Limite de requ√™tes atteinte',
                                details: 'Le service est temporairement indisponible. Veuillez r√©essayer plus tard.'
                            },
                            { status: 429 }
                        )
                    }

                    throw error
                }
            } else {
                chat = model.startChat({
                    history: [
                        {
                            role: 'user',
                            parts: [{ text: SYSTEM_PROMPT }],
                        },
                        {
                            role: 'model',
                            parts: [{ text: "Je suis pr√™t √† t'aider avec tes questions sur l'√©nergie." }],
                        },
                    ],
                    generationConfig: {
                        maxOutputTokens: 2048,
                    },
                })

                try {
                    const prompt = `
                    Contexte des donn√©es actuelles:
                    ${JSON.stringify(context)}
                    
                    Question de l'utilisateur: ${message}
                    `

                    console.log('üí≠ Prompt envoy√© √† Gemini:', {
                        question: message,
                        contextDonn√©es: context.buildings.currentConsumption
                    })

                    const result = await chat.sendMessage(prompt)
                    const response = await result.response.text()

                    if (!chatId) {
                        newChatId = Date.now().toString()
                        activeChats.set(newChatId, chat)
                    }

                    return NextResponse.json({
                        response,
                        chatId: newChatId
                    })
                } catch (error: any) {
                    console.error('Erreur Gemini (nouveau chat):', error.message)
                    throw error
                }
            }
        } catch (error: any) {
            console.error('Erreur d√©taill√©e Gemini:', error)

            // Gestion globale de l'erreur de quota
            if (error.message?.includes('429') || error.message?.includes('quota')) {
                return NextResponse.json(
                    {
                        error: 'Limite de requ√™tes atteinte',
                        details: 'Le service est temporairement indisponible. Veuillez r√©essayer plus tard.'
                    },
                    { status: 429 }
                )
            }

            return NextResponse.json(
                {
                    error: 'Erreur lors de la communication avec l\'assistant',
                    details: error.message
                },
                { status: 500 }
            )
        }
    } catch (error: any) {
        console.error('Erreur API Chat:', error)
        return NextResponse.json(
            {
                error: 'Erreur lors du traitement de la requ√™te',
                details: error.message
            },
            { status: 500 }
        )
    }
} 