import { GoogleGenerativeAI } from '@google/generative-ai'
import { NextResponse } from 'next/server'

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')

const SYSTEM_PROMPT = `Tu es un assistant intelligent sp√©cialis√© dans les sujets li√©s √† l'√©nergie et √† l'√©lectricit√©. Ton r√¥le est de r√©pondre aux questions des utilisateurs de mani√®re p√©dagogique, pr√©cise et utile. Les utilisateurs peuvent te poser des questions sur la production d'√©nergie, la consommation, les √©conomies d'√©nergie, les sources renouvelables, ou tout autre sujet li√© √† l'√©lectricit√© dans sa globalit√©.

Tu es aussi chaleureux et amical. Si un utilisateur te pose une question personnelle ou g√©n√©rale, tu peux r√©pondre bri√®vement de mani√®re sympathique avant de rediriger la conversation vers ton domaine d'expertise.

R√®gles importantes:
- R√©pondre de mani√®re amicale aux questions g√©n√©rales tout en redirigeant vers l'√©nergie
- Utiliser les donn√©es de consommation/production fournies quand c'est pertinent
- Rester chaleureux et professionnel
- Utiliser des √©mojis avec mod√©ration
- R√©pondre en fran√ßais
- √ätre concis et direct dans les r√©ponses techniques

Si une question est compl√®tement hors sujet, rappelle poliment que tu es sp√©cialis√© dans l'√©nergie et l'√©lectricit√©.`

export async function POST(req: Request) {
    try {
        const { message, context } = await req.json()

        // Utilisation de Gemini 1.5 Flash
        const model = genAI.getGenerativeModel({
            model: 'gemini-1.5-pro',
            generationConfig: {
                temperature: 0.7,
                topK: 1,
                topP: 0.8,
                maxOutputTokens: 2048,
            },
        })

        const chat = model.startChat({
            history: [
                {
                    role: 'user',
                    parts: [{ text: SYSTEM_PROMPT }],
                },
                {
                    role: 'model',
                    parts: [{ text: "Compris ! Je suis pr√™t √† discuter d'√©nergie et d'√©lectricit√© de mani√®re amicale et informative üòä" }],
                },
            ],
            generationConfig: {
                maxOutputTokens: 2048,
            },
        })

        const prompt = `
        Contexte des donn√©es actuelles:
        ${JSON.stringify(context)}
        
        Question de l'utilisateur: ${message}
        `

        const result = await chat.sendMessage(prompt)
        const response = await result.response.text()

        return NextResponse.json({ response })
    } catch (error) {
        console.error('Erreur API Chat:', error)
        return NextResponse.json(
            { error: 'Erreur lors du traitement de la demande' },
            { status: 500 }
        )
    }
} 